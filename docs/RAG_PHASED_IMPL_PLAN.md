**IMPORTANT P.S.**
We decided to **copy the entire course directory‚Äîexcept for the video files‚Äîfrom its original location on the D: drive into the project‚Äôs `course_content/` folder**. This avoids risks such as accidental modification of the original materials, slow HDD read speeds, and dependency on external paths. This dataset preparation step should be completed **by the end of Phase 0**, ensuring that all subsequent phases operate exclusively on the local project copy.

---

PS:
Project root directory is at:
`/home/akbar/Jupyter_Notebooks/python-course-rag`

---

# üéØ Phased Implementation Plan: Python Course RAG System

**Project**: Local Semantic Search for 100+ Hour Python Video Course  
**Timeline**: 7 days to working prototype  
**Philosophy**: Baseline first, incremental validation, rollback-friendly

---

## üìã Pre-Implementation: Environment Setup (Day 0, 1 hour)

### Goal

**Verify repository structure and establish baseline understanding before any changes.**

### Tasks

**0.1: Repository Exploration (20 min)**

```bash
# Navigate to course directory
cd /mnt/d/PYTHON_TUTORIALS/

# Verify structure matches expectations
ls -la "Python 3 Deep Dive (Part 1 - Functional)"/01*
ls -la "Python 3 Deep Dive (Part 2 - Iteration, Generators)"/01*

# Count total files
find . -name "*.srt" | wc -l
find . -name "*.mp4" | wc -l
find . -name "*.zip" | wc -l
find . -name "*.pdf" | wc -l
```

**0.2: Sample File Inspection (20 min)**

```bash
# Check SRT format
head -20 "Python 3 Deep Dive (Part 1 - Functional)/02 - A Quick Refresher - Basics Review/001 Introduction_en.srt"

# Check notebook structure (extract one .zip)
unzip -l "Python 3 Deep Dive (Part 1 - Functional)/02 - A Quick Refresher - Basics Review/003 Multi-Line-Statements-and-Strings.zip"

# Verify naming convention consistency
ls "Python 3 Deep Dive (Part 1 - Functional)"/02*/*.srt | head -10
```

**0.3: Create Project Workspace (20 min)**

```bash
# Create project directory separate from course files
mkdir -p ~/python-course-rag
cd ~/python-course-rag

# Create initial structure
mkdir -p {data,src,tests,docs}
touch docs/SETUP_LOG.md

# Initialize git
git init
echo "venv/" > .gitignore
echo "__pycache__/" >> .gitignore
echo "*.db" >> .gitignore
echo ".env" >> .gitignore
```

### Deliverables

- ‚úÖ Course structure documented in `docs/COURSE_STRUCTURE.md`
- ‚úÖ Sample files extracted and examined
- ‚úÖ Project workspace initialized
- ‚úÖ Git repository ready

### Success Criteria

- ‚úÖ Can navigate course directory structure
- ‚úÖ Naming convention is consistent (3-digit prefix + title)
- ‚úÖ SRT files are plain text and parseable
- ‚úÖ File counts match expectations (~1600 files)

### Time Estimate: 1 hour

---

## üéØ Phase 1: SRT Parsing Proof-of-Concept (Day 1, 3-4 hours)

### Goal

**Prove we can extract meaningful text from SRT files and preserve timestamp metadata‚Äîno database, no embeddings yet.**

### What We're Testing

- Can we parse SRT format reliably?
- Does subtitle text make sense for semantic search?
- Can we map text chunks back to video timestamps?
- What's the quality of content (typos, formatting issues)?

### Tasks

**1.1: Create SRT Parser (60 min)**

**File: `src/srt_parser.py`**

```python
"""
SRT subtitle parser with timestamp preservation.
"""
from dataclasses import dataclass
from datetime import timedelta
import re
from typing import List
from pathlib import Path


@dataclass
class SubtitleBlock:
    """Single subtitle entry with timing."""
    index: int
    start_time: timedelta
    end_time: timedelta
    text: str

    def to_seconds(self) -> tuple[float, float]:
        """Convert timedeltas to seconds for easier handling."""
        return (
            self.start_time.total_seconds(),
            self.end_time.total_seconds()
        )


def parse_srt(file_path: Path) -> List[SubtitleBlock]:
    """
    Parse SRT file into structured subtitle blocks.

    Args:
        file_path: Path to .srt file

    Returns:
        List of SubtitleBlock objects

    Raises:
        ValueError: If SRT format is invalid
    """
    blocks = []

    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # Split by double newline (subtitle separator)
    raw_blocks = re.split(r'\n\s*\n', content.strip())

    for raw_block in raw_blocks:
        lines = raw_block.strip().split('\n')
        if len(lines) < 3:
            continue  # Skip malformed blocks

        try:
            # Parse index
            index = int(lines[0])

            # Parse timestamps
            # Format: 00:00:01,000 --> 00:00:03,500
            time_match = re.match(
                r'(\d{2}):(\d{2}):(\d{2}),(\d{3})\s*-->\s*(\d{2}):(\d{2}):(\d{2}),(\d{3})',
                lines[1]
            )

            if not time_match:
                continue

            start_time = timedelta(
                hours=int(time_match.group(1)),
                minutes=int(time_match.group(2)),
                seconds=int(time_match.group(3)),
                milliseconds=int(time_match.group(4))
            )

            end_time = timedelta(
                hours=int(time_match.group(5)),
                minutes=int(time_match.group(6)),
                seconds=int(time_match.group(7)),
                milliseconds=int(time_match.group(8))
            )

            # Join remaining lines as text
            text = ' '.join(lines[2:])

            blocks.append(SubtitleBlock(
                index=index,
                start_time=start_time,
                end_time=end_time,
                text=text
            ))

        except (ValueError, IndexError) as e:
            print(f"Warning: Skipping malformed block in {file_path}: {e}")
            continue

    return blocks


def group_blocks_by_time(
    blocks: List[SubtitleBlock],
    max_chars: int = 1000
) -> List[dict]:
    """
    Group subtitle blocks into semantic chunks.

    Strategy: Combine blocks until max_chars reached or natural break detected.

    Args:
        blocks: List of SubtitleBlock objects
        max_chars: Maximum characters per chunk

    Returns:
        List of dicts with keys: text, start_time, end_time
    """
    chunks = []
    current_chunk_text = []
    current_chunk_start = None
    current_chunk_end = None
    current_length = 0

    for block in blocks:
        if current_chunk_start is None:
            current_chunk_start = block.start_time

        current_chunk_text.append(block.text)
        current_chunk_end = block.end_time
        current_length += len(block.text)

        # Create chunk if size threshold reached
        if current_length >= max_chars:
            chunks.append({
                'text': ' '.join(current_chunk_text),
                'start_time': current_chunk_start.total_seconds(),
                'end_time': current_chunk_end.total_seconds()
            })

            # Reset for next chunk
            current_chunk_text = []
            current_chunk_start = None
            current_length = 0

    # Handle final chunk
    if current_chunk_text:
        chunks.append({
            'text': ' '.join(current_chunk_text),
            'start_time': current_chunk_start.total_seconds(),
            'end_time': current_chunk_end.total_seconds()
        })

    return chunks
```

**1.2: Create Test Script (30 min)**

**File: `tests/test_srt_parser.py`**

```python
"""
Test SRT parser with real course files.
"""
from pathlib import Path
from src.srt_parser import parse_srt, group_blocks_by_time


def test_single_file():
    """Test parsing a single SRT file."""
    # Use a known file from the course
    test_file = Path("/mnt/d/PYTHON_TUTORIALS/Python 3 Deep Dive (Part 1 - Functional)/02 - A Quick Refresher - Basics Review/001 Introduction_en.srt")

    if not test_file.exists():
        print(f"‚ùå Test file not found: {test_file}")
        return

    print(f"‚úÖ Testing: {test_file.name}")

    # Parse file
    blocks = parse_srt(test_file)
    print(f"  üìä Parsed {len(blocks)} subtitle blocks")

    # Show first 3 blocks
    for block in blocks[:3]:
        start, end = block.to_seconds()
        print(f"  [{start:.1f}s - {end:.1f}s]: {block.text[:80]}...")

    # Group into chunks
    chunks = group_blocks_by_time(blocks, max_chars=500)
    print(f"\n  üì¶ Created {len(chunks)} chunks")

    # Show first chunk
    chunk = chunks[0]
    print(f"  First chunk ({chunk['start_time']:.1f}s - {chunk['end_time']:.1f}s):")
    print(f"    {chunk['text'][:200]}...")

    return blocks, chunks


def test_multiple_files():
    """Test parsing multiple files to check consistency."""
    course_root = Path("/mnt/d/PYTHON_TUTORIALS/Python 3 Deep Dive (Part 1 - Functional)")

    # Find first 5 SRT files
    srt_files = list(course_root.rglob("*.srt"))[:5]

    print(f"\nüîÑ Testing {len(srt_files)} files for consistency...")

    results = []
    for srt_file in srt_files:
        try:
            blocks = parse_srt(srt_file)
            chunks = group_blocks_by_time(blocks, max_chars=1000)
            results.append({
                'file': srt_file.name,
                'blocks': len(blocks),
                'chunks': len(chunks),
                'success': True
            })
            print(f"  ‚úÖ {srt_file.name}: {len(blocks)} blocks ‚Üí {len(chunks)} chunks")
        except Exception as e:
            results.append({
                'file': srt_file.name,
                'error': str(e),
                'success': False
            })
            print(f"  ‚ùå {srt_file.name}: {e}")

    success_rate = sum(r['success'] for r in results) / len(results) * 100
    print(f"\n  Success rate: {success_rate:.0f}%")

    return results


if __name__ == '__main__':
    print("=" * 60)
    print("SRT PARSER TEST SUITE")
    print("=" * 60)

    # Test 1: Single file
    test_single_file()

    # Test 2: Multiple files
    test_multiple_files()

    print("\n" + "=" * 60)
    print("TESTS COMPLETE")
    print("=" * 60)
```

**1.3: Run Tests and Document Findings (30 min)**

```bash
cd ~/python-course-rag

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Run tests
python tests/test_srt_parser.py > docs/PHASE1_RESULTS.txt
cat docs/PHASE1_RESULTS.txt
```

**1.4: Quality Assessment (45 min)**

Create `tests/quality_check.py`:

```python
"""
Assess SRT content quality for semantic search.
"""
from pathlib import Path
from src.srt_parser import parse_srt, group_blocks_by_time
import re


def assess_content_quality(chunks: list) -> dict:
    """
    Analyze chunk quality for semantic search.

    Returns dict with metrics:
    - avg_length: Average characters per chunk
    - contains_code: Percentage mentioning code
    - has_questions: Percentage with questions
    - readability: Estimated readability (simple heuristic)
    """
    metrics = {
        'total_chunks': len(chunks),
        'avg_length': sum(len(c['text']) for c in chunks) / len(chunks),
        'min_length': min(len(c['text']) for c in chunks),
        'max_length': max(len(c['text']) for c in chunks),
        'contains_code': 0,
        'has_questions': 0,
        'code_terms': ['function', 'variable', 'class', 'method', 'return', 'def', 'import']
    }

    for chunk in chunks:
        text_lower = chunk['text'].lower()

        # Check for code-related terms
        if any(term in text_lower for term in metrics['code_terms']):
            metrics['contains_code'] += 1

        # Check for questions
        if '?' in chunk['text']:
            metrics['has_questions'] += 1

    metrics['code_percentage'] = (metrics['contains_code'] / len(chunks)) * 100
    metrics['question_percentage'] = (metrics['has_questions'] / len(chunks)) * 100

    return metrics


if __name__ == '__main__':
    # Test on multiple files
    course_root = Path("/mnt/d/PYTHON_TUTORIALS/Python 3 Deep Dive (Part 1 - Functional)")
    srt_files = list(course_root.rglob("*.srt"))[:10]

    print("CONTENT QUALITY ASSESSMENT")
    print("=" * 60)

    all_chunks = []
    for srt_file in srt_files:
        blocks = parse_srt(srt_file)
        chunks = group_blocks_by_time(blocks, max_chars=1000)
        all_chunks.extend(chunks)

    metrics = assess_content_quality(all_chunks)

    print(f"\nAnalyzed {len(srt_files)} files, {metrics['total_chunks']} chunks")
    print(f"Average chunk length: {metrics['avg_length']:.0f} chars")
    print(f"Range: {metrics['min_length']} - {metrics['max_length']} chars")
    print(f"Code-related content: {metrics['code_percentage']:.1f}%")
    print(f"Contains questions: {metrics['question_percentage']:.1f}%")

    # Sample chunks
    print("\n" + "=" * 60)
    print("SAMPLE CHUNKS")
    print("=" * 60)
    for i, chunk in enumerate(all_chunks[:3], 1):
        print(f"\nChunk {i} ({chunk['start_time']:.1f}s):")
        print(chunk['text'][:300] + "...")
```

**1.5: Document Findings and Decide (30 min)**

Update `docs/PHASE1_RESULTS.md` with:

- Parse success rate
- Average chunk sizes
- Content quality assessment
- Any SRT format issues encountered
- Decision: Proceed to Phase 2 or adjust chunking strategy

### Deliverables

- ‚úÖ `src/srt_parser.py` - Working SRT parser
- ‚úÖ `tests/test_srt_parser.py` - Test suite
- ‚úÖ `tests/quality_check.py` - Quality assessment
- ‚úÖ `docs/PHASE1_RESULTS.md` - Analysis and findings

### Success Criteria

- ‚úÖ Can parse 95%+ of SRT files without errors
- ‚úÖ Chunks are 500-1500 characters (suitable for embeddings)
- ‚úÖ Timestamps correctly preserved
- ‚úÖ Content contains meaningful information (not just "um", "uh")
- ‚úÖ Can map chunks back to specific video segments

### Rollback Plan

If SRT parsing unreliable:

1. Investigate specific failure cases
2. Add error handling for malformed blocks
3. Consider alternative: use video metadata + manual snippet creation

### Time Estimate: 3-4 hours

---

## üéØ Phase 2: Embedding Model Selection (Day 2, 2-3 hours)

### Goal

**Test embedding models on sample course content‚Äîfind best balance of speed, quality, and size for CPU execution.**

### What We're Testing

- Embedding generation speed on CPU
- Model disk size (must fit locally)
- Search quality with course-specific queries
- Memory usage during embedding

### Tasks

**2.1: Install Embedding Libraries (15 min)**

```bash
pip install sentence-transformers
pip install chromadb  # Temporary, just for testing embeddings
```

**2.2: Create Embedding Benchmark Script (60 min)**

**File: `src/embedding_benchmark.py`**

```python
"""
Benchmark embedding models on CPU with course content.
"""
from sentence_transformers import SentenceTransformer
import time
from pathlib import Path
from src.srt_parser import parse_srt, group_blocks_by_time
import numpy as np


# Models to test (from GDR recommendations)
MODELS_TO_TEST = [
    'BAAI/bge-small-en-v1.5',  # 384 dimensions, fastest
    'all-MiniLM-L6-v2',        # 384 dimensions, widely used
    'all-MiniLM-L12-v2',       # 384 dimensions, more accurate
]


def load_sample_chunks(n_files: int = 5) -> list[str]:
    """Load sample chunks from course for testing."""
    course_root = Path("/mnt/d/PYTHON_TUTORIALS/Python 3 Deep Dive (Part 1 - Functional)")
    srt_files = list(course_root.rglob("*.srt"))[:n_files]

    all_chunks = []
    for srt_file in srt_files:
        blocks = parse_srt(srt_file)
        chunks = group_blocks_by_time(blocks, max_chars=1000)
        all_chunks.extend([c['text'] for c in chunks])

    return all_chunks[:50]  # Limit to 50 chunks for testing


def benchmark_model(model_name: str, texts: list[str]) -> dict:
    """
    Benchmark a single embedding model.

    Returns:
        dict with metrics: load_time, embed_time, dimensions, memory_mb
    """
    print(f"\n{'='*60}")
    print(f"Testing: {model_name}")
    print(f"{'='*60}")

    # Load model
    start = time.time()
    model = SentenceTransformer(model_name)
    load_time = time.time() - start
    print(f"‚úÖ Model loaded in {load_time:.2f}s")

    # Embed texts
    start = time.time()
    embeddings = model.encode(texts, show_progress_bar=True)
    embed_time = time.time() - start

    avg_time_per_chunk = embed_time / len(texts)
    chunks_per_second = len(texts) / embed_time

    print(f"‚úÖ Embedded {len(texts)} chunks in {embed_time:.2f}s")
    print(f"   Average: {avg_time_per_chunk:.3f}s per chunk")
    print(f"   Speed: {chunks_per_second:.1f} chunks/second")
    print(f"   Dimensions: {embeddings.shape[1]}")

    # Estimate time for full course (assuming 5000 chunks)
    estimated_full_time = (5000 / chunks_per_second) / 60
    print(f"   Estimated full course: {estimated_full_time:.1f} minutes")

    return {
        'model_name': model_name,
        'load_time': load_time,
        'embed_time': embed_time,
        'avg_time_per_chunk': avg_time_per_chunk,
        'chunks_per_second': chunks_per_second,
        'dimensions': embeddings.shape[1],
        'estimated_full_course_minutes': estimated_full_time
    }


def test_search_quality(model_name: str, texts: list[str]):
    """
    Test search quality with sample queries.
    """
    print(f"\n{'='*60}")
    print(f"Search Quality Test: {model_name}")
    print(f"{'='*60}")

    model = SentenceTransformer(model_name)
    embeddings = model.encode(texts, show_progress_bar=False)

    # Test queries (representative of actual use)
    test_queries = [
        "How do I handle exceptions in Python?",
        "What are decorators and how do they work?",
        "Explain list comprehensions with examples",
        "How does memory management work in Python?",
        "What is the difference between args and kwargs?"
    ]

    for query in test_queries:
        query_embedding = model.encode([query])[0]

        # Calculate cosine similarity
        similarities = np.dot(embeddings, query_embedding) / (
            np.linalg.norm(embeddings, axis=1) * np.linalg.norm(query_embedding)
        )

        # Get top 3 results
        top_indices = np.argsort(similarities)[-3:][::-1]

        print(f"\n‚ùì Query: {query}")
        for i, idx in enumerate(top_indices, 1):
            print(f"  {i}. (Score: {similarities[idx]:.3f}) {texts[idx][:100]}...")


if __name__ == '__main__':
    print("EMBEDDING MODEL BENCHMARK")
    print("="*60)

    # Load sample data
    print("\nüìÅ Loading sample chunks...")
    texts = load_sample_chunks(n_files=5)
    print(f"‚úÖ Loaded {len(texts)} chunks for testing\n")

    # Benchmark each model
    results = []
    for model_name in MODELS_TO_TEST:
        try:
            result = benchmark_model(model_name, texts)
            results.append(result)
        except Exception as e:
            print(f"‚ùå Error with {model_name}: {e}")

    # Summary comparison
    print("\n" + "="*60)
    print("SUMMARY COMPARISON")
    print("="*60)

    for result in results:
        print(f"\n{result['model_name']}:")
        print(f"  Speed: {result['chunks_per_second']:.1f} chunks/sec")
        print(f"  Dimensions: {result['dimensions']}")
        print(f"  Full course estimate: {result['estimated_full_course_minutes']:.1f} min")

    # Test search quality on best performer
    if results:
        fastest = min(results, key=lambda x: x['avg_time_per_chunk'])
        print(f"\nüèÜ Testing search quality on fastest model: {fastest['model_name']}")
        test_search_quality(fastest['model_name'], texts)
```

**2.3: Run Benchmark (30 min)**

```bash
python src/embedding_benchmark.py > docs/PHASE2_BENCHMARK.txt
```

**2.4: Analyze Results and Choose Model (30 min)**

Create decision matrix in `docs/PHASE2_DECISION.md`:

```markdown
# Embedding Model Selection

## Test Results

| Model             | Speed (chunks/sec) | Dimensions | Full Course Time | Search Quality |
| ----------------- | ------------------ | ---------- | ---------------- | -------------- |
| bge-small-en-v1.5 | [FILL]             | 384        | [FILL] min       | [FILL]         |
| all-MiniLM-L6-v2  | [FILL]             | 384        | [FILL] min       | [FILL]         |
| all-MiniLM-L12-v2 | [FILL]             | 384        | [FILL] min       | [FILL]         |

## Decision Criteria

1. **Speed**: Must embed 5000 chunks in <30 minutes on CPU
2. **Quality**: Test queries return relevant results
3. **Size**: Model must fit in <2GB disk space

## Selected Model: [CHOICE]

**Justification:**

- [Speed metric]
- [Quality assessment]
- [Trade-offs considered]
```

### Deliverables

- ‚úÖ `src/embedding_benchmark.py` - Benchmark script
- ‚úÖ `docs/PHASE2_BENCHMARK.txt` - Raw benchmark results
- ‚úÖ `docs/PHASE2_DECISION.md` - Model selection rationale

### Success Criteria

- ‚úÖ At least one model generates embeddings <0.1s per chunk on CPU
- ‚úÖ Search quality test returns relevant results for sample queries
- ‚úÖ Chosen model can process full course in <30 minutes
- ‚úÖ Model size <2GB

### Rollback Plan

If no model meets speed requirements:

1. **Option A**: Use Google Colab for one-time embedding generation (export vectors)
2. **Option B**: Reduce corpus size (only most important sections)
3. **Option C**: Use pre-computed embeddings from similar educational content

### Time Estimate: 2-3 hours

---

<!-- jjj -->

## üéØ Phase 3: LanceDB Integration & Full Indexing (Day 3, 3-4 hours)

### Goal

**Build production indexing pipeline: process all SRT files, generate embeddings, store in LanceDB. Validate on sample query.**

### What We're Testing

- Can we generate 5000+ embeddings on CPU without running out of RAM?
- Does LanceDB handle the full dataset efficiently?
- Can we preserve timestamps through the full pipeline?
- Query latency acceptable (<3 seconds)?

### Tasks

**3.1: Install LanceDB & Dependencies (10 min)**

```bash
cd ~/python-course-rag
source venv/bin/activate

pip install lancedb
pip install llama-index-core==0.10.67
pip install llama-index-embeddings-huggingface
pip install llama-index-vector-stores-lancedb
pip install pyarrow

# Verify installations
python -c "import lancedb; import llama_index; print('‚úÖ All deps installed')"
```

**3.2: Create SRT-to-Documents Processor (60 min)**

**File: `src/srt_processor.py`**

```python
"""
Convert SRT files into LlamaIndex Documents with timestamp metadata.
Implements time-based chunking strategy from GEM_3.md.
"""
import srt
from pathlib import Path
from llama_index.core import Document
from typing import List
import re


def extract_video_metadata(srt_path: Path, course_root: Path) -> dict:
    """
    Extract metadata from file path structure.

    Expected: /PYTHON_TUTORIALS/[Course Name]/[Section]/[###] Title_en.srt
    """
    # Extract course name (contains "Deep Dive")
    course_name = None
    for part in srt_path.parts:
        if "Deep Dive" in part:
            course_name = part
            break

    # Extract section (immediate parent directory)
    section = srt_path.parent.name

    # Extract video ID and title from filename
    filename = srt_path.stem.replace('_en', '')
    match = re.match(r'(\d+)\s+(.+)', filename)

    if match:
        video_id = match.group(1)
        video_title = match.group(2)
    else:
        video_id = "unknown"
        video_title = filename

    return {
        'course': course_name or "Unknown Course",
        'section': section,
        'video_id': video_id,
        'video_title': video_title,
        'file_path': str(srt_path)
    }


def srt_to_documents(
    srt_path: Path,
    course_root: Path,
    max_chars: int = 1000
) -> List[Document]:
    """
    Parse SRT file and create LlamaIndex Documents with timestamp metadata.

    Strategy: Group subtitle blocks until reaching max_chars, preserving
    the start/end time of each group for video timestamp mapping.

    Args:
        srt_path: Path to .srt file
        course_root: Root directory for metadata extraction
        max_chars: Target characters per chunk

    Returns:
        List of Document objects with metadata
    """
    try:
        with open(srt_path, 'r', encoding='utf-8') as f:
            blocks = list(srt.parse(f.read()))
    except Exception as e:
        print(f"‚ö†Ô∏è  Error reading {srt_path}: {e}")
        return []

    if not blocks:
        print(f"‚ö†Ô∏è  No subtitles found in {srt_path}")
        return []

    # Extract metadata once
    metadata = extract_video_metadata(srt_path, course_root)

    documents = []
    current_group = []
    current_length = 0

    for block in blocks:
        # Clean subtitle text (remove HTML tags, extra whitespace)
        text = block.content.replace('\n', ' ').strip()
        text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags if any

        if not text:
            continue

        current_group.append(block)
        current_length += len(text)

        # Create document when threshold reached
        if current_length >= max_chars:
            full_text = " ".join([
                re.sub(r'<[^>]+>', '', b.content.replace('\n', ' ').strip())
                for b in current_group
            ])

            doc = Document(
                text=full_text,
                metadata={
                    **metadata,  # Spread base metadata
                    'start_time_seconds': current_group[0].start.total_seconds(),
                    'end_time_seconds': current_group[-1].end.total_seconds(),
                    'chunk_index': len(documents),
                    'file_type': 'srt'
                }
            )
            documents.append(doc)
            current_group = []
            current_length = 0

    # Handle final partial chunk
    if current_group:
        full_text = " ".join([
            re.sub(r'<[^>]+>', '', b.content.replace('\n', ' ').strip())
            for b in current_group
        ])

        doc = Document(
            text=full_text,
            metadata={
                **metadata,
                'start_time_seconds': current_group[0].start.total_seconds(),
                'end_time_seconds': current_group[-1].end.total_seconds(),
                'chunk_index': len(documents),
                'file_type': 'srt'
            }
        )
        documents.append(doc)

    return documents


def process_all_srt_files(
    course_root: Path,
    max_chars: int = 1000,
    limit: int = None
) -> List[Document]:
    """
    Recursively find and process all SRT files in course directory.

    Args:
        course_root: Root of course directory
        max_chars: Chunk size parameter
        limit: For testing, process only first N files (None = all)

    Returns:
        Combined list of all documents
    """
    srt_files = sorted(course_root.rglob("*_en.srt"))

    if limit:
        srt_files = srt_files[:limit]

    all_documents = []

    for i, srt_file in enumerate(srt_files, 1):
        print(f"[{i}/{len(srt_files)}] Processing {srt_file.parent.name}/{srt_file.name}...", end=" ")

        docs = srt_to_documents(srt_file, course_root, max_chars)
        all_documents.extend(docs)

        print(f"‚úÖ {len(docs)} chunks")

    return all_documents
```

**3.3: Create Indexing Script (45 min)**

**File: `src/build_index.py`**

```python
"""
Build LanceDB index from processed SRT documents.
Configures embedding model and storage, then runs full indexing.
"""
from pathlib import Path
from llama_index.core import VectorStoreIndex, StorageContext, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.lancedb import LanceDBVectorStore
from src.srt_processor import process_all_srt_files
import time


def build_index(
    course_root: Path,
    db_path: Path = Path("./data/lancedb"),
    embedding_model: str = "BAAI/bge-small-en-v1.5",
    max_chars: int = 1000,
    limit_files: int = None
):
    """
    Build complete LanceDB index from course SRT files.

    Args:
        course_root: Path to course directory
        db_path: Where to store LanceDB data
        embedding_model: HuggingFace model name
        max_chars: Chunk size
        limit_files: For testing, limit to N files (None = all)
    """
    print("=" * 70)
    print("BUILDING LANCEDB INDEX")
    print("=" * 70)

    # Step 1: Configure embedding model
    print("\nüì¶ Configuring embedding model...")
    print(f"   Model: {embedding_model}")
    print(f"   Dimensions: 384")
    print(f"   Strategy: CPU-optimized, fast inference")

    Settings.embed_model = HuggingFaceEmbedding(
        model_name=embedding_model,
        trust_remote_code=False
    )

    # Step 2: Load and process all SRT files
    print(f"\nüìÇ Loading SRT files from {course_root}")
    start_time = time.time()

    documents = process_all_srt_files(
        course_root,
        max_chars=max_chars,
        limit=limit_files
    )

    load_time = time.time() - start_time

    print(f"\n‚úÖ Loaded {len(documents)} document chunks in {load_time:.1f}s")

    if not documents:
        print("‚ùå No documents to index!")
        return None

    # Step 3: Setup LanceDB storage
    print(f"\nüíæ Initializing LanceDB at {db_path}")
    db_path.parent.mkdir(parents=True, exist_ok=True)

    vector_store = LanceDBVectorStore(
        uri=str(db_path),
        table_name="course_content",
        mode="overwrite"  # Fresh index each time
    )
    storage_context = StorageContext.from_defaults(vector_store=vector_store)

    # Step 4: Generate embeddings and build index
    print(f"\nüöÄ Generating embeddings and building index...")
    print(f"   (This may take 5-60 minutes depending on document count)")

    embed_start = time.time()

    index = VectorStoreIndex.from_documents(
        documents,
        storage_context=storage_context,
        show_progress=True
    )

    embed_time = time.time() - embed_start

    print(f"\n‚úÖ Index built in {embed_time:.1f}s")
    print(f"   Avg time per document: {embed_time/len(documents):.3f}s")

    # Step 5: Print statistics
    print("\n" + "=" * 70)
    print("INDEX STATISTICS")
    print("=" * 70)
    print(f"Total documents indexed: {len(documents)}")
    print(f"Database location: {db_path}")
    print(f"Database size (estimate): {(len(documents) * 384 * 4) / (1024**2):.1f} MB")

    return index


def main():
    """Entry point for building index."""
    # Configuration
    COURSE_ROOT = Path("/mnt/d/PYTHON_TUTORIALS/Python 3 Deep Dive (Part 1 - Functional)")
    DB_PATH = Path("./data/lancedb")
    EMBEDDING_MODEL = "BAAI/bge-small-en-v1.5"
    MAX_CHARS = 1000
    LIMIT_FILES = None  # Change to 5 for testing on subset

    # Validate course path exists
    if not COURSE_ROOT.exists():
        print(f"‚ùå Course path not found: {COURSE_ROOT}")
        return

    # Build index
    index = build_index(
        course_root=COURSE_ROOT,
        db_path=DB_PATH,
        embedding_model=EMBEDDING_MODEL,
        max_chars=MAX_CHARS,
        limit_files=LIMIT_FILES
    )

    if index:
        print(f"\n‚úÖ Index ready at {DB_PATH}")
        print("   Next: Run tests/test_search.py to validate retrieval")


if __name__ == "__main__":
    main()
```

**3.4: Test Indexing on Sample (30 min)**

**File: `tests/test_indexing.py`**

```python
"""
Validate indexing pipeline on small sample before full run.
"""
from pathlib import Path
from src.build_index import build_index
import sys


def test_sample_indexing():
    """Test on 3 SRT files to validate pipeline."""
    print("üß™ INDEXING VALIDATION TEST")
    print("=" * 70)

    COURSE_ROOT = Path("/mnt/d/PYTHON_TUTORIALS/Python 3 Deep Dive (Part 1 - Functional)")
    TEST_DB = Path("./data/lancedb_test")

    # Clean up previous test
    import shutil
    if TEST_DB.exists():
        shutil.rmtree(TEST_DB)

    # Build small index
    print("\nüìù Building test index on 3 files...")
    index = build_index(
        course_root=COURSE_ROOT,
        db_path=TEST_DB,
        embedding_model="BAAI/bge-small-en-v1.5",
        max_chars=1000,
        limit_files=3  # Only 3 files for fast testing
    )

    if not index:
        print("‚ùå Index creation failed!")
        return False

    # Test basic query
    print("\nüîç Testing sample query...")
    query_engine = index.as_query_engine(similarity_top_k=5)

    test_query = "How do I use functions in Python?"

    try:
        response = query_engine.query(test_query)

        print(f"\nQuery: '{test_query}'")
        print(f"\n‚úÖ Got {len(response.source_nodes)} results:\n")

        for i, node in enumerate(response.source_nodes, 1):
            meta = node.metadata
            start_time = int(meta['start_time_seconds'])

            timestamp = f"{start_time // 60}:{start_time % 60:02d}"

            print(f"{i}. [{meta['video_title']}] @ {timestamp}")
            print(f"   Score: {node.score:.3f}")
            print(f"   Text: {node.text[:100]}...")
            print()

        print("‚úÖ Indexing and search working!")
        return True

    except Exception as e:
        print(f"‚ùå Query failed: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = test_sample_indexing()
    sys.exit(0 if success else 1)
```

**3.5: Run Full Indexing (60+ min, mostly waiting)**

```bash
cd ~/python-course-rag

# First: Test on small sample
echo "Testing on 3 files..."
python tests/test_indexing.py

# If successful, run full indexing
# Adjust LIMIT_FILES to None in src/build_index.py main()
echo "Building full index (this will take 30-90 minutes)..."
python src/build_index.py | tee docs/PHASE3_INDEXING_LOG.txt
```

### Deliverables

- ‚úÖ `src/srt_processor.py` - SRT to Document converter
- ‚úÖ `src/build_index.py` - Indexing orchestration
- ‚úÖ `tests/test_indexing.py` - Sample validation
- ‚úÖ `data/lancedb/` - Production vector store (created after running)
- ‚úÖ `docs/PHASE3_INDEXING_LOG.txt` - Indexing statistics

### Success Criteria

- ‚úÖ Can process 3 test files without errors in <5 minutes
- ‚úÖ Sample query returns 5+ relevant results with correct timestamps
- ‚úÖ Full indexing completes without OOM errors
- ‚úÖ LanceDB file created (<3GB for full 100+ hour course)
- ‚úÖ Query latency <3 seconds

### Rollback Plan

If embeddings take >2 hours:

1. Use Google Colab to generate embeddings (faster GPU)
2. Export embeddings + metadata as Arrow format
3. Import into local LanceDB (skip embedding generation)

If LanceDB file corruption:

1. Delete `data/lancedb/` directory
2. Rerun indexing script (will recreate)

### Time Estimate: 3-4 hours (mostly waiting for embedding generation)

---

## üéØ Phase 4: Query Engine & Reranking (Day 4, 2-3 hours)

### Goal

**Implement two-stage retrieval (vector search + cross-encoder reranking) for superior result quality.**

### What We're Testing

- Does reranking significantly improve result relevance?
- Can we measure quality objectively?
- Query latency still acceptable with reranking?

### Tasks

**4.1: Install Cross-Encoder (5 min)**

```bash
pip install sentence-transformers
```

**4.2: Create Query Engine with Reranking (45 min)**

**File: `src/query_engine.py`**

```python
"""
Query engine with two-stage retrieval: vector search + cross-encoder reranking.
"""
from pathlib import Path
from llama_index.core import VectorStoreIndex, StorageContext, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.lancedb import LanceDBVectorStore
from llama_index.core.postprocessors import SentenceTransformerRerank
from typing import List, Dict, Optional
import time


class CourseQueryEngine:
    """Semantic search engine for course content with reranking."""

    def __init__(
        self,
        db_path: Path = Path("./data/lancedb"),
        embedding_model: str = "BAAI/bge-small-en-v1.5",
        rerank_model: str = "cross-encoder/ms-marco-MiniLM-L-6-v2",
        use_reranking: bool = True
    ):
        """
        Initialize query engine.

        Args:
            db_path: Path to LanceDB directory
            embedding_model: HuggingFace embedding model
            rerank_model: HuggingFace cross-encoder model
            use_reranking: Whether to apply reranking stage
        """
        self.db_path = Path(db_path)
        self.use_reranking = use_reranking

        # Configure embedding model
        Settings.embed_model = HuggingFaceEmbedding(
            model_name=embedding_model,
            trust_remote_code=False
        )

        # Load existing index from LanceDB
        print("üìñ Loading existing index from LanceDB...")
        vector_store = LanceDBVectorStore(
            uri=str(self.db_path),
            table_name="course_content"
        )
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        self.index = VectorStoreIndex.from_vector_store(
            vector_store=vector_store,
            storage_context=storage_context
        )
        print("‚úÖ Index loaded")

        # Setup reranker if enabled
        if self.use_reranking:
            print(f"üìä Loading reranker: {rerank_model}")
            self.reranker = SentenceTransformerRerank(
                model=rerank_model,
                top_n=5  # Keep only top 5 after reranking
            )
            print("‚úÖ Reranker loaded")
        else:
            self.reranker = None

    def query(
        self,
        query_text: str,
        top_k: int = 10,
        verbose: bool = False
    ) -> Dict:
        """
        Execute semantic search with optional reranking.

        Args:
            query_text: Natural language query
            top_k: Number of candidates to retrieve before reranking
            verbose: Print timing information

        Returns:
            Dict with 'results' list and metadata
        """
        start_time = time.time()

        # Create query engine
        if self.reranker and self.use_reranking:
            query_engine = self.index.as_query_engine(
                similarity_top_k=top_k,
                node_postprocessors=[self.reranker]
            )
        else:
            query_engine = self.index.as_query_engine(
                similarity_top_k=top_k
            )

        # Execute query
        response = query_engine.query(query_text)
        query_time = time.time() - start_time

        # Format results
        results = []
        for i, node in enumerate(response.source_nodes, 1):
            meta = node.metadata
            start_seconds = int(meta['start_time_seconds'])
            end_seconds = int(meta['end_time_seconds'])

            # Format timestamp for display
            start_ts = f"{start_seconds // 60}:{start_seconds % 60:02d}"
            end_ts = f"{end_seconds // 60}:{end_seconds % 60:02d}"

            results.append({
                'rank': i,
                'video_id': meta['video_id'],
                'video_title': meta['video_title'],
                'section': meta['section'],
                'start_time_seconds': start_seconds,
                'end_time_seconds': end_seconds,
                'start_timestamp': start_ts,
                'end_timestamp': end_ts,
                'score': float(node.score),
                'text_snippet': node.text[:300],
                'full_text': node.text
            })

        if verbose:
            print(f"Query time: {query_time:.2f}s")
            print(f"Results returned: {len(results)}")

        return {
            'query': query_text,
            'results': results,
            'query_time': query_time,
            'total_results': len(results),
            'reranking_used': self.use_reranking
        }

    def compare_with_without_reranking(self, query_text: str, top_k: int = 10):
        """
        Run query both with and without reranking for comparison.
        Useful for validating reranking effectiveness.
        """
        print("\n" + "=" * 70)
        print(f"RERANKING COMPARISON: '{query_text}'")
        print("=" * 70)

        # Without reranking
        self.use_reranking = False
        results_no_rerank = self.query(query_text, top_k=top_k)

        # With reranking
        self.use_reranking = True
        results_with_rerank = self.query(query_text, top_k=top_k)

        print(f"\nüìä WITHOUT reranking ({len(results_no_rerank['results'])} results):")
        for r in results_no_rerank['results'][:5]:
            print(f"  {r['rank']}. {r['video_title']} @ {r['start_timestamp']} (score: {r['score']:.3f})")

        print(f"\nüìä WITH reranking ({len(results_with_rerank['results'])} results):")
        for r in results_with_rerank['results'][:5]:
            print(f"  {r['rank']}. {r['video_title']} @ {r['start_timestamp']} (score: {r['score']:.3f})")

        # Reset to reranking enabled
        self.use_reranking = True

        return {
            'without': results_no_rerank,
            'with': results_with_rerank
        }
```

**4.3: Create Query Testing Script (45 min)**

**File: `tests/test_queries.py`**

```python
"""
Test query engine with representative sample queries.
"""
from pathlib import Path
from src.query_engine import CourseQueryEngine
import json


# Representative test queries (based on typical learning scenarios)
TEST_QUERIES = [
    "How do I raise and handle exceptions?",
    "Explain list comprehensions with examples",
    "What are decorators and how do I use them?",
    "How does memory management work in Python?",
    "What's the difference between args and kwargs?",
    "How do I work with generators?",
    "Explain the difference between class and static methods",
    "How do I create and use context managers?",
]


def test_query_engine():
    """Test query engine on multiple queries."""
    print("üß™ QUERY ENGINE TEST")
    print("=" * 70)

    DB_PATH = Path("./data/lancedb")

    # Validate database exists
    if not DB_PATH.exists():
        print(f"‚ùå Database not found at {DB_PATH}")
        print("   Run: python src/build_index.py")
        return False

    # Initialize engine
    print("\nüîß Initializing query engine...")
    try:
        engine = CourseQueryEngine(
            db_path=DB_PATH,
            use_reranking=True
        )
    except Exception as e:
        print(f"‚ùå Failed to initialize engine: {e}")
        return False

    # Run test queries
    print(f"\nüîç Running {len(TEST_QUERIES)} test queries...")
    print("=" * 70)

    results_summary = []

    for i, query in enumerate(TEST_QUERIES, 1):
        print(f"\n[{i}/{len(TEST_QUERIES)}] Query: '{query}'")

        try:
            result = engine.query(query, top_k=10, verbose=True)

            print(f"\nüìä Top 3 Results:")
            for res in result['results'][:3]:
                print(f"  {res['rank']}. {res['video_title']} (score: {res['score']:.3f})")
                print(f"     @ {res['start_timestamp']} - {res['end_timestamp']}")
                print(f"     {res['text_snippet']}...")

            results_summary.append({
                'query': query,
                'num_results': result['total_results'],
                'top_score': result['results'][0]['score'] if result['results'] else 0,
                'query_time': result['query_time']
            })

        except Exception as e:
            print(f"‚ùå Query failed: {e}")
            results_summary.append({
                'query': query,
                'error': str(e)
            })

    # Print summary
    print("\n" + "=" * 70)
    print("TEST SUMMARY")
    print("=" * 70)

    successful = len([r for r in results_summary if 'error' not in r])
    avg_time = sum(r.get('query_time', 0) for r in results_summary) / successful if successful > 0 else 0

    print(f"\n‚úÖ Successful queries: {successful}/{len(TEST_QUERIES)}")
    print(f"‚è±Ô∏è  Average query time: {avg_time:.2f}s")

    # Save results
    output_file = Path("./docs/PHASE4_QUERY_RESULTS.json")
    with open(output_file, 'w') as f:
        json.dump(results_summary, f, indent=2)

    print(f"\nüìÑ Results saved to {output_file}")

    return successful == len(TEST_QUERIES)


def compare_reranking_effectiveness():
    """Compare results with and without reranking."""
    print("\nüß™ RERANKING EFFECTIVENESS COMPARISON")
    print("=" * 70)

    DB_PATH = Path("./data/lancedb")
    engine = CourseQueryEngine(db_path=DB_PATH, use_reranking=True)

    # Test queries where reranking should make most difference
    queries = [
        "How do I use list comprehensions?",
        "Explain decorators and how they work",
    ]

    for query in queries:
        comparison = engine.compare_with_without_reranking(query, top_k=10)


if __name__ == "__main__":
    success = test_query_engine()

    if success:
        print("\n‚úÖ Basic query testing passed. Testing reranking...")
        compare_reranking_effectiveness()

    print("\n‚úÖ Query engine validation complete!")
```

**4.4: Run Query Tests (30 min)**

```bash
cd ~/python-course-rag

# Test the query engine
python tests/test_queries.py 2>&1 | tee docs/PHASE4_TEST_RESULTS.txt

# Review results
cat docs/PHASE4_QUERY_RESULTS.json | head -50
```

### Deliverables

- ‚úÖ `src/query_engine.py` - Query engine with reranking
- ‚úÖ `tests/test_queries.py` - Comprehensive query tests
- ‚úÖ `docs/PHASE4_QUERY_RESULTS.json` - Test results data
- ‚úÖ `docs/PHASE4_TEST_RESULTS.txt` - Detailed test output

### Success Criteria

- ‚úÖ Query engine initializes successfully
- ‚úÖ 8/8 test queries return results (no errors)
- ‚úÖ Query latency <3 seconds (ideally <1s with reranking)
- ‚úÖ Reranking visibly improves top result relevance
- ‚úÖ Can manually verify top result is semantically appropriate

### Rollback Plan

If reranking is too slow:

1. Reduce `top_n` parameter (currently 5, try 3)
2. Skip reranking entirely (still get good results from vector search)
3. Use lighter cross-encoder model

### Time Estimate: 2-3 hours

---

## üéØ Phase 5: User Interface & Optimization (Days 5-6, 4-5 hours)

### Goal

**Create CLI + optional Streamlit UI; optimize for production use.**

### What We're Testing

- CLI interface intuitive and responsive?
- Streamlit UI loads/runs smoothly?
- UI displays timestamps correctly for manual video lookup?

### Tasks

**5.1: Create CLI Interface (45 min)**

**File: `src/cli.py`**

```python
"""
Command-line interface for course semantic search.
"""
from pathlib import Path
from src.query_engine import CourseQueryEngine
from typing import Optional
import click
from rich.console import Console
from rich.table import Table


console = Console()


class CourseSearchCLI:
    """CLI for interactive course search."""

    def __init__(self, db_path: Path = Path("./data/lancedb")):
        """Initialize CLI with query engine."""
        self.engine = CourseQueryEngine(db_path=db_path, use_reranking=True)

    def format_result(self, result: dict, index: int) -> str:
        """Format single search result for display."""
        return f"""
[bold cyan]{index}. {result['video_title']}[/bold cyan]
   üìç Section: {result['section']}
   ‚è±Ô∏è  Timestamp: {result['start_timestamp']} - {result['end_timestamp']}
   üìä Relevance Score: {result['score']:.3f}
   üìù {result['text_snippet']}...
"""

    def search(self, query: str, limit: int = 5):
        """Execute search and display results."""
        console.print(f"\n[bold]Searching:[/bold] {query}\n", style="blue")

        try:
            result = self.engine.query(query, top_k=limit, verbose=False)

            if not result['results']:
                console.print("[yellow]‚ùå No results found[/yellow]")
                return

            console.print(f"[green]‚úÖ Found {result['total_results']} results in {result['query_time']:.2f}s[/green]\n")

            # Display results
            for i, res in enumerate(result['results'], 1):
                console.print(self.format_result(res, i))

            # Print copy-paste friendly format
            console.print("[bold]üìã Quick Reference (copy timestamps):[/bold]")
            for res in result['results'][:3]:
                console.print(f"  {res['video_id']}: {res['start_timestamp']}")

        except Exception as e:
            console.print(f"[red]‚ùå Error: {e}[/red]")


@click.group()
def cli():
    """Python Course Semantic Search CLI."""
    pass


@cli.command()
@click.option('--query', '-q', prompt='Enter search query', help='Search query')
@click.option('--limit', '-l', default=5, help='Number of results to show')
@click.option('--db-path', default='./data/lancedb', help='Path to LanceDB')
def search(query: str, limit: int, db_path: str):
    """Search course content."""
    cli_engine = CourseSearchCLI(db_path=Path(db_path))
    cli_engine.search(query, limit=limit)


@cli.command()
@click.option('--db-path', default='./data/lancedb', help='Path to LanceDB')
def interactive(db_path: str):
    """Start interactive search mode."""
    cli_engine = CourseSearchCLI(db_path=Path(db_path))

    console.print("[bold cyan]üéì Python Course Search - Interactive Mode[/bold cyan]")
    console.print("[dim]Type 'exit' or 'quit' to stop[/dim]\n")

    while True:
        try:
            query = click.prompt("\nüîç Query", default="", show_default=False)

            if query.lower() in ['exit', 'quit', 'q']:
                console.print("[yellow]Goodbye![/yellow]")
                break

            if not query.strip():
                continue

            cli_engine.search(query, limit=5)

        except KeyboardInterrupt:
            console.print("\n[yellow]Interrupted[/yellow]")
            break


@cli.command()
@click.option('--db-path', default='./data/lancedb', help='Path to LanceDB')
def stats(db_path: str):
    """Show database statistics."""
    db_path = Path(db_path)

    if not db_path.exists():
        console.print(f"[red]‚ùå Database not found at {db_path}[/red]")
        return

    try:
        engine = CourseQueryEngine(db_path=db_path)

        console.print("\n[bold cyan]üìä Database Statistics[/bold cyan]\n")

        table = Table(show_header=True, header_style="bold")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="green")

        table.add_row("Database Location", str(db_path))
        table.add_row("Embedding Model", "BAAI/bge-small-en-v1.5")
        table.add_row("Vector Dimensions", "384")
        table.add_row("Reranking Model", "cross-encoder/ms-marco-MiniLM-L-6-v2")

        console.print(table)

    except Exception as e:
        console.print(f"[red]‚ùå Error: {e}[/red]")


if __name__ == "__main__":
    cli()
```

**5.2: Create Streamlit Web Interface (90 min)**

**File: `app.py`** (in project root)

```python
"""
Streamlit web interface for course semantic search.
Run with: streamlit run app.py
"""
import streamlit as st
from pathlib import Path
from src.query_engine import CourseQueryEngine
import json


# Page configuration
st.set_page_config(
    page_title="üéì Python Course Search",
    page_icon="üéì",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better styling
st.markdown("""
<style>
    .result-card {
        border: 1px solid #ddd;
        border-radius: 8px;
        padding: 16px;
        margin: 10px 0;
        background-color: #f9f9f9;
    }
    .timestamp-badge {
        display: inline-block;
        background-color: #ff6b6b;
        color: white;
        padding: 4px 12px;
        border-radius: 20px;
        font-weight: bold;
        margin: 5px 5px 5px 0;
    }
    .score-badge {
        display: inline-block;
        background-color: #4ecdc4;
        color: white;
        padding: 4px 12px;
        border-radius: 20px;
        font-size: 0.9em;
    }
</style>
""", unsafe_allow_html=True)


@st.cache_resource
def load_query_engine():
    """Load query engine (cached)."""
    db_path = Path("./data/lancedb")

    if not db_path.exists():
        st.error(f"‚ùå Database not found at {db_path}")
        st.info("Run: `python src/build_index.py` to build the index first")
        st.stop()

    try:
        engine = CourseQueryEngine(
            db_path=db_path,
            use_reranking=True
        )
        return engine
    except Exception as e:
        st.error(f"Failed to load query engine: {e}")
        st.stop()


def render_search_results(results: list, query: str):
    """Render search results as cards."""
    if not results:
        st.warning("No results found. Try a different query.")
        return

    st.success(f"‚úÖ Found {len(results)} results")

    for i, result in enumerate(results, 1):
        with st.container():
            col1, col2 = st.columns([3, 1])

            with col1:
                st.markdown(f"### {i}. {result['video_title']}")
                st.markdown(f"**Section:** {result['section']}")

            with col2:
                score_pct = int(result['score'] * 100)
                st.metric("Relevance", f"{score_pct}%")

            # Timestamp and section info
            col1, col2, col3 = st.columns([2, 2, 1])

            with col1:
                st.write(f"**Start:** `{result['start_timestamp']}`")
            with col2:
                st.write(f"**End:** `{result['end_timestamp']}`")
            with col3:
                st.write(f"**ID:** `{result['video_id']}`")

            # Text snippet
            st.markdown(f"**Snippet:**\n> {result['text_snippet']}...")

            # Copy-paste friendly timestamp
            st.code(f"Video: {result['video_id']} | Time: {result['start_timestamp']}",
                   language="text")

            st.divider()


def main():
    """Main Streamlit app."""
    # Header
    st.title("üéì Python Course Semantic Search")
    st.markdown("Search your course content with natural language queries")

    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è Settings")

        num_results = st.slider(
            "Number of results to return",
            min_value=1,
            max_value=20,
            value=5,
            step=1
        )

        show_full_text = st.checkbox("Show full text snippets", value=False)

        st.divider()

        st.subheader("‚ÑπÔ∏è About")
        st.markdown("""
        This tool uses semantic search to find relevant content in your
        Python course. It combines:

        - **Vector embeddings** for semantic understanding
        - **Cross-encoder reranking** for result quality
        - **Timestamp preservation** for direct video navigation

        **Stack:**
        - Embedding: `BAAI/bge-small-en-v1.5`
        - Reranker: `cross-encoder/ms-marco-MiniLM-L-6-v2`
        - Database: LanceDB
        """)

    # Main search interface
    st.divider()

    # Load engine
    engine = load_query_engine()

    # Search input
    search_query = st.text_input(
        "Enter your search query",
        placeholder="e.g., 'How do I use decorators in Python?'",
        label_visibility="collapsed"
    )

    # Example queries
    with st.expander("üìå Example Queries"):
        example_queries = [
            "How do I raise and handle exceptions?",
            "Explain list comprehensions",
            "What are decorators?",
            "How does memory management work?",
            "Difference between args and kwargs",
        ]

        cols = st.columns(2)
        for i, eq in enumerate(example_queries):
            with cols[i % 2]:
                if st.button(eq, key=f"example_{i}"):
                    search_query = eq

    # Execute search
    if search_query:
        with st.spinner("üîç Searching..."):
            try:
                result = engine.query(search_query, top_k=num_results, verbose=False)

                # Display metrics
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Results Found", result['total_results'])
                with col2:
                    st.metric("Query Time", f"{result['query_time']:.2f}s")
                with col3:
                    status = "‚úÖ With Reranking" if result['reranking_used'] else "üîç Vector Search Only"
                    st.metric("Mode", status)

                st.divider()

                # Render results
                render_search_results(result['results'], search_query)

            except Exception as e:
                st.error(f"‚ùå Search failed: {e}")
                st.exception(e)

    else:
        # Show welcome message
        st.info("""
        üëã **Welcome!**

        Enter a search query above to get started. Try questions like:
        - "How do I use list comprehensions?"
        - "Explain decorators with examples"
        - "What's the difference between classes and namedtuples?"
        """)


if __name__ == "__main__":
    main()
```

**5.3: Create Entry Point Scripts (30 min)**

**File: `run_cli.sh`**

```bash
#!/bin/bash
# Quick start CLI interface

cd ~/python-course-rag
source venv/bin/activate

echo "üéì Python Course Search - CLI"
echo "============================"
echo ""
echo "Choose mode:"
echo "  1) Single query"
echo "  2) Interactive mode"
echo "  3) Database stats"
echo ""
read -p "Enter choice (1-3): " choice

case $choice in
  1)
    read -p "Enter query: " query
    python -m src.cli search --query "$query"
    ;;
  2)
    python -m src.cli interactive
    ;;
  3)
    python -m src.cli stats
    ;;
  *)
    echo "Invalid choice"
    ;;
esac
```

**File: `run_web.sh`**

```bash
#!/bin/bash
# Quick start Streamlit interface

cd ~/python-course-rag
source venv/bin/activate

echo "üéì Python Course Search - Web Interface"
echo "======================================="
echo ""
echo "Starting Streamlit app..."
echo "Open browser to: http://localhost:8501"
echo ""

streamlit run app.py
```

**5.4: Update Requirements (15 min)**

**File: `requirements.txt`**

```
# Core RAG Stack
llama-index-core==0.10.67
llama-index-embeddings-huggingface==0.1.0
llama-index-vector-stores-lancedb==0.1.0
lancedb>=0.3.0

# Embedding and Reranking
sentence-transformers>=2.2.0
torch>=2.0.0

# Data Processing
srt>=0.0.1
pyarrow>=12.0.0

# CLI and Web UI
click>=8.1.0
rich>=13.0.0
streamlit>=1.28.0

# Utilities
python-dotenv>=1.0.0
```

**5.5: Create Usage Documentation (30 min)**

**File: `docs/USAGE_GUIDE.md`**

````markdown
# Usage Guide: Python Course Semantic Search

## Quick Start

### 1. Build the Index (First Time Only)

```bash
cd ~/python-course-rag
source venv/bin/activate

# Build index on 5 files (testing)
# Edit src/build_index.py, set LIMIT_FILES=5
python src/build_index.py

# Build full index (all files)
# Edit src/build_index.py, set LIMIT_FILES=None
python src/build_index.py  # Takes 30-90 minutes
```
````

### 2. CLI Interface (Recommended for Quick Searches)

```bash
# Single query
./run_cli.sh
# Select option 1, enter query

# Interactive mode (multiple queries)
./run_cli.sh
# Select option 2

# Example:
# Query: "How do I use decorators?"
# Results show timestamps like 03:45 where content appears
```

### 3. Web Interface (Recommended for Browsing)

```bash
./run_web.sh
# Opens http://localhost:8501 in browser
```

## Understanding Results

Each result shows:

| Field                | Meaning                                |
| -------------------- | -------------------------------------- |
| Video Title          | Name of the video segment              |
| Section              | Course section (e.g., "Section 8")     |
| Start/End Timestamps | When in the video this content appears |
| Relevance Score      | 0-100, higher = better match           |
| Snippet              | First 300 characters of content        |

## Timestamp Navigation

Results display timestamps like `03:45`. To jump to that location:

### Option 1: VLC Player

```bash
# Start video at specific time (in seconds)
vlc "video.mp4" --start-time=225  # 225 sec = 3:45
```

### Option 2: FFmpeg (Extract Clip)

```bash
# Extract 10-second clip starting at timestamp
ffmpeg -i video.mp4 -ss 03:45 -t 10 -c copy clip.mp4
```

### Option 3: Manual Navigation

Use the timestamp shown in results to manually seek in your video player.

## Query Examples

### Well-Structured Queries (Will Work Well)

- "How do I use list comprehensions?"
- "Explain the difference between args and kwargs"
- "Show me examples of decorators in Python"
- "What's a context manager and when should I use it?"

### Vague Queries (Will Return Broad Results)

- "Python"
- "code"
- "example"

**Tip:** More specific queries = better results.

## Troubleshooting

### Q: Database not found

**A:** Run `python src/build_index.py` first

### Q: Queries too slow (>5 seconds)

**A:**

1. Check CPU usage (might be indexing in background)
2. Reduce `top_k` parameter in query settings
3. Consider disabling reranking temporarily

### Q: Irrelevant results

**A:**

1. Try more specific queries
2. Reranking is active; if still bad, submit feedback
3. Check if content actually exists in course

## Performance Tuning

### Faster Queries (Less Accurate)

Edit `src/query_engine.py`, set `top_n=3`:

```python
self.reranker = SentenceTransformerRerank(
    model=rerank_model,
    top_n=3  # Was 5, now faster
)
```

### More Accurate Results (Slower Queries)

Edit `src/query_engine.py`, set `top_n=10`:

```python
self.reranker = SentenceTransformerRerank(
    model=rerank_model,
    top_n=10  # More reranking candidates
)
```

## Rebuilding the Index

If you add new course content:

```bash
# Backup current index
cp -r data/lancedb data/lancedb_backup

# Rebuild
python src/build_index.py

# The script will process ALL SRT files (including new ones)
```

## Advanced: Adding Content Sources

### Add Jupyter Notebooks (Optional)

```python
# In src/srt_processor.py, add notebook processing

from llama_index.core import SimpleDirectoryReader

# After SRT processing:
notebooks = SimpleDirectoryReader("./notebooks").load_data()
all_documents.extend(notebooks)
```

### Add PDF Slides

```python
# In src/srt_processor.py

from llama_index.core.readers import PDFReader

# Process PDFs
pdf_reader = PDFReader()
pdfs = pdf_reader.load_data(directory="./slides/")
all_documents.extend(pdfs)
```

## Support & Debugging

### Enable Debug Logging

```python
# In app.py or cli.py, add at top:
import logging
logging.basicConfig(level=logging.DEBUG)
```

### Export Results

```bash
# CLI: Results are shown in terminal, copy as needed

# Web: Right-click > Save results to clipboard
```

### Report Issues

Create `docs/DEBUG_REPORT.md` with:

1. Query you ran
2. Expected vs actual results
3. Relevant timestamp
4. System info (RAM, CPU)

````

**5.6: Test All Interfaces (45 min)**

```bash
cd ~/python-course-rag

# Test CLI
echo "Testing CLI..."
python -m src.cli search -q "list comprehension" -l 3

# Test interactive
echo "Testing interactive (type 'exit' after first query)..."
echo -e "decorators\nexit" | python -m src.cli interactive

# Test stats
python -m src.cli stats

# Test Streamlit (requires manual browser check)
echo "Starting Streamlit for manual testing..."
timeout 30 streamlit run app.py 2>&1 | head -20
````

### Deliverables

- ‚úÖ `src/cli.py` - Full CLI implementation
- ‚úÖ `app.py` - Streamlit web interface
- ‚úÖ `run_cli.sh` - CLI launcher script
- ‚úÖ `run_web.sh` - Web launcher script
- ‚úÖ `requirements.txt` - Updated dependencies
- ‚úÖ `docs/USAGE_GUIDE.md` - Complete usage documentation

### Success Criteria

- ‚úÖ CLI search works and returns formatted results
- ‚úÖ Interactive mode allows multiple queries
- ‚úÖ Streamlit interface loads and runs queries
- ‚úÖ Timestamps display correctly in both interfaces
- ‚úÖ All interfaces handle errors gracefully

### Rollback Plan

If Streamlit has issues:

- CLI is fully functional fallback
- Can always query programmatically via `src/query_engine.py`

### Time Estimate: 4-5 hours

---

<!-- jjj -->

## üéØ Phase 6: Validation & Documentation (Day 7, 2-3 hours)

### Goal

**Comprehensive validation, performance benchmarking, prepare for replication on new courses.**

### Tasks

**6.1: Create Validation Test Suite (45 min)**

**File: `tests/validation.py`**

```python
"""
Comprehensive validation suite to ensure system quality.
"""
from pathlib import Path
from src.query_engine import CourseQueryEngine
import json
import time


def create_validation_queries() -> list[dict]:
    """
    Create test queries with known relevance.
    Format: {query, should_contain, section}
    """
    return [
        {
            "query": "How do I use list comprehensions in Python?",
            "should_contain": ["comprehension", "list"],
            "description": "Basic list comprehension query"
        },
        {
            "query": "Explain decorators with real examples",
            "should_contain": ["decorator", "@"],
            "description": "Decorators query"
        },
        {
            "query": "What's the difference between args and kwargs?",
            "should_contain": ["args", "kwargs", "*"],
            "description": "Args/kwargs query"
        },
        {
            "query": "Exception handling best practices",
            "should_contain": ["exception", "try", "except"],
            "description": "Exception handling query"
        },
        {
            "query": "Context managers and the with statement",
            "should_contain": ["context", "with", "__enter__"],
            "description": "Context manager query"
        }
    ]


def validate_result_quality(result: dict, test_case: dict) -> dict:
    """
    Validate if top result matches expected content.
    """
    top_result = result['results'][0] if result['results'] else None

    if not top_result:
        return {
            'passed': False,
            'reason': 'No results returned',
            'score': 0
        }

    text = (top_result['full_text'] or "").lower()

    # Check if should_contain terms appear in result
    matches = sum(1 for term in test_case['should_contain']
                 if term.lower() in text)

    match_rate = matches / len(test_case['should_contain'])

    passed = match_rate >= 0.5  # At least 50% of terms

    return {
        'passed': passed,
        'match_rate': match_rate,
        'top_result_score': top_result['score'],
        'video_id': top_result['video_id'],
        'reason': f"Matched {matches}/{len(test_case['should_contain'])} keywords"
    }


def run_validation_suite():
    """Run full validation suite."""
    print("üß™ VALIDATION TEST SUITE")
    print("=" * 70)

    db_path = Path("./data/lancedb")

    if not db_path.exists():
        print(f"‚ùå Database not found at {db_path}")
        return False

    engine = CourseQueryEngine(db_path=db_path, use_reranking=True)

    test_cases = create_validation_queries()
    results_log = []

    print(f"\nRunning {len(test_cases)} validation queries...\n")

    for i, test_case in enumerate(test_cases, 1):
        print(f"[{i}/{len(test_cases)}] {test_case['description']}")
        print(f"  Query: '{test_case['query']}'")

        start = time.time()
        result = engine.query(test_case['query'], top_k=5)
        elapsed = time.time() - start

        validation = validate_result_quality(result, test_case)
        validation['query'] = test_case['query']
        validation['query_time'] = elapsed

        status = "‚úÖ" if validation['passed'] else "‚ö†Ô∏è"
        print(f"  {status} {validation['reason']}")
        print(f"     Time: {elapsed:.2f}s | Top Score: {validation['top_result_score']:.3f}")

        results_log.append(validation)
        print()

    # Summary
    passed = sum(1 for r in results_log if r['passed'])

    print("=" * 70)
    print("VALIDATION SUMMARY")
    print("=" * 70)
    print(f"Passed: {passed}/{len(test_cases)} ({100*passed/len(test_cases):.0f}%)")

    avg_time = sum(r['query_time'] for r in results_log) / len(results_log)
    print(f"Avg Query Time: {avg_time:.2f}s")

    # Save results
    output_file = Path("./docs/VALIDATION_RESULTS.json")
    with open(output_file, 'w') as f:
        json.dump(results_log, f, indent=2)

    print(f"\nüìÑ Full results saved to {output_file}")

    return passed >= len(test_cases) * 0.8  # Pass if 80%+ pass


if __name__ == "__main__":
    success = run_validation_suite()
    print(f"\n{'‚úÖ VALIDATION PASSED' if success else '‚ö†Ô∏è VALIDATION INCONCLUSIVE'}")
```

**6.2: Create Performance Benchmark (30 min)**

**File: `tests/benchmark.py`**

```python
"""
Performance benchmarking for production readiness.
"""
from pathlib import Path
from src.query_engine import CourseQueryEngine
import time
import statistics


def benchmark_queries():
    """Benchmark query performance."""
    print("‚ö° QUERY PERFORMANCE BENCHMARK")
    print("=" * 70)

    db_path = Path("./data/lancedb")
    engine = CourseQueryEngine(db_path=db_path, use_reranking=True)

    queries = [
        "list comprehension",
        "decorators",
        "exception handling",
        "async await",
        "context managers"
    ]

    times = []

    print(f"\nRunning {len(queries)} queries 5 times each...\n")

    for query in queries:
        query_times = []

        for run in range(5):
            start = time.time()
            engine.query(query, top_k=5)
            elapsed = time.time() - start
            query_times.append(elapsed)

        avg = statistics.mean(query_times)
        stdev = statistics.stdev(query_times) if len(query_times) > 1 else 0

        times.extend(query_times)

        print(f"'{query}':")
        print(f"  Avg: {avg:.3f}s | Stdev: {stdev:.3f}s | Min: {min(query_times):.3f}s | Max: {max(query_times):.3f}s")

    # Overall stats
    print("\n" + "=" * 70)
    print("OVERALL PERFORMANCE")
    print("=" * 70)
    print(f"Total queries: {len(times)}")
    print(f"Avg time per query: {statistics.mean(times):.3f}s")
    print(f"Median time: {statistics.median(times):.3f}s")
    print(f"95th percentile: {sorted(times)[int(len(times)*0.95)]:.3f}s")
    print(f"Max time: {max(times):.3f}s")

    # Performance evaluation
    avg_time = statistics.mean(times)
    if avg_time < 1.0:
        print(f"\n‚úÖ EXCELLENT: Queries <1s (interactive-grade performance)")
    elif avg_time < 3.0:
        print(f"\n‚úÖ GOOD: Queries <3s (acceptable performance)")
    else:
        print(f"\n‚ö†Ô∏è FAIR: Queries >{avg_time:.1f}s (consider optimizations)")


if __name__ == "__main__":
    benchmark_queries()
```

**6.3: Create Replication Guide (45 min)**

**File: `docs/REPLICATION_GUIDE.md`**

```markdown
# Replication Guide: Apply Pipeline to New Courses

This guide explains how to use this RAG system on another course with minimal changes.

## Prerequisites

- Course organized with structure similar to original
- Subtitle files (`.srt`) for each video
- ~30-90 minutes for initial indexing (depending on course size)

## Step 1: Organize Course Files

Place course in standard structure:
```

/path/to/new_course/
‚îú‚îÄ‚îÄ Section 1/
‚îÇ ‚îú‚îÄ‚îÄ 001_Video_Title_en.srt
‚îÇ ‚îú‚îÄ‚îÄ 002_Video_Title_en.srt
‚îÇ ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ Section 2/
‚îÇ ‚îú‚îÄ‚îÄ 001_Video_Title_en.srt
‚îÇ ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ ...

````

**Key:** Files must end with `_en.srt` for detection.

## Step 2: Update Configuration

Edit `src/build_index.py`:

```python
def main():
    # Change this line:
    COURSE_ROOT = Path("/path/to/new_course")

    # Everything else stays the same!
    DB_PATH = Path("./data/lancedb")
    EMBEDDING_MODEL = "BAAI/bge-small-en-v1.5"
    MAX_CHARS = 1000
    LIMIT_FILES = None
````

## Step 3: Build New Index

```bash
cd ~/python-course-rag
python src/build_index.py
# Takes 30-90 minutes depending on course size
```

## Step 4: Test

```bash
# Single query
python -m src.cli search -q "your query"

# Or web interface
./run_web.sh
```

## Step 5: Optimize (Optional)

### Adjust Chunk Size

If results are too granular:

```python
# In src/build_index.py
build_index(..., max_chars=1500)  # Increase from 1000
```

If results are too broad:

```python
build_index(..., max_chars=500)  # Decrease from 1000
```

### Adjust Reranking

If results are good without reranking:

```python
# In src/query_engine.py
engine = CourseQueryEngine(use_reranking=False)
```

### Change Embedding Model

For faster (but less accurate) embeddings:

```python
# In src/build_index.py
EMBEDDING_MODEL = "all-MiniLM-L6-v2"  # Faster alternative
```

## Troubleshooting

### Issue: Out of memory during indexing

**Solution:** Process in batches

```python
# In src/build_index.py
build_index(..., limit_files=50)  # Build in chunks
```

### Issue: SRT files not detected

**Solution:** Check naming convention

```bash
# Files must end with _en.srt
# Rename if needed:
for f in *.srt; do mv "$f" "${f%.srt}_en.srt"; done
```

### Issue: Poor search results

**Solution:** Validate content quality

```bash
python tests/validation.py
# Check if subtitles contain meaningful content
```

## Configuration Summary

| Parameter         | Default              | When to Change              |
| ----------------- | -------------------- | --------------------------- |
| `COURSE_ROOT`     | Original course path | **Always** (for new course) |
| `MAX_CHARS`       | 1000                 | Results too broad/granular  |
| `EMBEDDING_MODEL` | `bge-small-en-v1.5`  | Need faster/better quality  |
| `use_reranking`   | `True`               | Queries too slow            |
| `top_k`           | 10                   | Want more/fewer candidates  |

## Expected Results

After following this guide, you should have:

- ‚úÖ Working search on new course
- ‚úÖ Query latency <3 seconds
- ‚úÖ Relevant results for course-specific queries
- ‚úÖ Timestamp navigation to videos

## Time Investment

- **Initial Setup:** 10 minutes
- **Indexing:** 30-90 minutes (one-time)
- **Testing/Validation:** 15 minutes
- **Total:** ~1-2 hours

## Support

If issues persist:

1. Run validation suite: `python tests/validation.py`
2. Check logs in `docs/PHASE*_*.txt`
3. Verify SRT content quality manually

````

**6.4: Create Final Project README (45 min)**

**File: `README.md`** (in project root)

```markdown
# üéì Python Course Semantic Search System

Local RAG-based semantic search system for educational video courses. Built for 100+ hour Python course; easily replicable to other courses.

## üéØ What It Does

Allows natural language queries across course content:

```bash
Query: "How do I use decorators in Python?"

Results:
1. [Video 045] Python Decorators Explained @ 03:45
   Score: 0.92 | "A decorator is a function that wraps another function..."

2. [Video 047] Practical Decorator Examples @ 08:12
   Score: 0.87 | "Let me show you how decorators work in real code..."
````

## ‚ú® Features

- **Semantic Search**: Understands meaning, not just keywords
- **Timestamp Precision**: Jump directly to relevant video moments
- **Two-Stage Retrieval**: Vector search + cross-encoder reranking
- **Dual Interface**: CLI and web UI
- **Local & Private**: Runs entirely on your machine
- **Fast**: <3s query latency on CPU

## üöÄ Quick Start

### 1. Installation

```bash
# Clone or setup project directory
cd ~/python-course-rag

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Build Index (First Time Only)

```bash
# Test on small subset (5 files)
# Edit src/build_index.py: LIMIT_FILES = 5
python src/build_index.py  # ~2-5 minutes

# Build full index (all course content)
# Edit src/build_index.py: LIMIT_FILES = None
python src/build_index.py  # ~30-90 minutes
```

### 3. Search!

**CLI (Recommended for quick searches):**

```bash
./run_cli.sh
# Select option 2 for interactive mode
```

**Web Interface (Recommended for browsing):**

```bash
./run_web.sh
# Opens http://localhost:8501
```

## üìã Requirements

- Python 3.8+
- 8GB+ RAM (16GB recommended for large courses)
- ~3GB disk space for embeddings + vector database
- CPU (no GPU required, but speeds up if available)

## üèóÔ∏è Architecture

```
Course SRT Files
    ‚Üì
Parse & Chunk (time-based)
    ‚Üì
Generate Embeddings (bge-small-en-v1.5, 384-dim)
    ‚Üì
Store in LanceDB
    ‚Üì
Query ‚Üí Vector Search ‚Üí Cross-Encoder Rerank ‚Üí Results
```

### Tech Stack

| Component       | Technology                             |
| --------------- | -------------------------------------- |
| Embedding Model | `BAAI/bge-small-en-v1.5`               |
| Reranker        | `cross-encoder/ms-marco-MiniLM-L-6-v2` |
| Vector Database | LanceDB                                |
| Orchestration   | LlamaIndex                             |
| CLI             | Click + Rich                           |
| Web UI          | Streamlit                              |

## üìñ Documentation

- **[Usage Guide](docs/USAGE_GUIDE.md)**: How to search, interpret results, navigate timestamps
- **[Replication Guide](docs/REPLICATION_GUIDE.md)**: Apply to your own course (10-minute setup)
- **[Project Brief](docs/project_brief.md)**: Original requirements and motivation
- **[Implementation Plan](docs/RAG_PHASED_IMPL_PLAN.md)**: Phased development approach

## üéØ Use Cases

### For Students

- "Find where async/await is explained with examples"
- "Show me all videos about exception handling"
- "What did the instructor say about memory management?"

### For Instructors

- Validate content coverage
- Find specific teaching moments
- Cross-reference topics across sections

### For Developers

- Quick reference lookup while coding
- Review specific concepts without rewatching full videos
- Find implementation examples

## üß™ Testing & Validation

```bash
# Run validation suite
python tests/validation.py

# Performance benchmark
python tests/benchmark.py

# Test specific query
python -m src.cli search -q "your query"
```

## üìä Performance

Tested on: Intel i7-12700K, 64GB RAM, CPU-only

| Metric           | Result                |
| ---------------- | --------------------- |
| Index Build Time | ~45 min (5000 chunks) |
| Query Latency    | 0.8-2.5s              |
| Database Size    | ~2.8GB                |
| Memory Usage     | ~4GB during search    |

## üîß Customization

### Adjust Chunk Size

```python
# In src/build_index.py
build_index(..., max_chars=1500)  # Larger chunks
```

### Disable Reranking (Faster)

```python
# In src/query_engine.py
engine = CourseQueryEngine(use_reranking=False)
```

### Change Embedding Model

```python
# In src/build_index.py
EMBEDDING_MODEL = "all-MiniLM-L12-v2"  # More accurate
```

## üêõ Troubleshooting

### Database Not Found

```bash
# Solution: Build index first
python src/build_index.py
```

### Out of Memory

```bash
# Solution: Process in batches
# In src/build_index.py: LIMIT_FILES = 50
```

### Poor Results

```bash
# Solution: Validate content quality
python tests/validation.py
```

See **[Usage Guide](docs/USAGE_GUIDE.md)** for more troubleshooting.

## üõ†Ô∏è Project Structure

```
python-course-rag/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ srt_parser.py          # SRT parsing with timestamps
‚îÇ   ‚îú‚îÄ‚îÄ srt_processor.py       # Convert to LlamaIndex Documents
‚îÇ   ‚îú‚îÄ‚îÄ build_index.py         # Indexing orchestration
‚îÇ   ‚îú‚îÄ‚îÄ query_engine.py        # Search + reranking
‚îÇ   ‚îî‚îÄ‚îÄ cli.py                 # CLI interface
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_srt_parser.py     # Parser validation
‚îÇ   ‚îú‚îÄ‚îÄ test_indexing.py       # Index building test
‚îÇ   ‚îú‚îÄ‚îÄ test_queries.py        # Query engine test
‚îÇ   ‚îú‚îÄ‚îÄ validation.py          # Quality validation
‚îÇ   ‚îî‚îÄ‚îÄ benchmark.py           # Performance benchmark
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ USAGE_GUIDE.md         # User documentation
‚îÇ   ‚îú‚îÄ‚îÄ REPLICATION_GUIDE.md   # Setup for new courses
‚îÇ   ‚îî‚îÄ‚îÄ project_brief.md       # Original requirements
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ lancedb/               # Vector database (created after build)
‚îú‚îÄ‚îÄ app.py                     # Streamlit web interface
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îî‚îÄ‚îÄ README.md                  # This file
```

## üìù Development Timeline

Built in 7 days following phased approach:

- **Day 0**: Environment setup & exploration
- **Day 1**: SRT parsing proof-of-concept
- **Day 2**: Embedding model selection
- **Day 3**: Full indexing pipeline
- **Day 4**: Query engine with reranking
- **Days 5-6**: User interfaces
- **Day 7**: Validation & documentation

## üöÄ Future Enhancements

- [ ] Include Jupyter notebook content
- [ ] PDF slide extraction
- [ ] Multi-language subtitle support
- [ ] Auto-reindex when content added
- [ ] Query history and favorites
- [ ] Result export to markdown

## üìÑ License

This is a personal project. Reuse freely for educational purposes.

## üôè Acknowledgments

Built with:

- [LlamaIndex](https://github.com/run-llama/llama_index) - RAG orchestration
- [LanceDB](https://github.com/lancedb/lancedb) - Vector database
- [Sentence Transformers](https://github.com/UKPLab/sentence-transformers) - Embeddings
- [Streamlit](https://streamlit.io/) - Web interface

---

**Questions?** Check [docs/USAGE_GUIDE.md](docs/USAGE_GUIDE.md) or open an issue.

````

**6.5: Run Final Validation (30 min)**

```bash
cd ~/python-course-rag

echo "Running final validation suite..."

# 1. Validation tests
python tests/validation.py > docs/FINAL_VALIDATION.txt

# 2. Performance benchmark
python tests/benchmark.py > docs/FINAL_BENCHMARK.txt

# 3. CLI test
echo "Testing CLI..."
python -m src.cli search -q "list comprehension" -l 3

# 4. Check all docs exist
echo ""
echo "Documentation Check:"
for doc in docs/USAGE_GUIDE.md docs/REPLICATION_GUIDE.md README.md; do
    if [ -f "$doc" ]; then
        echo "‚úÖ $doc"
    else
        echo "‚ùå $doc MISSING"
    fi
done

echo ""
echo "Validation complete! Review docs/FINAL_*.txt for results."
````

### Deliverables

- ‚úÖ `tests/validation.py` - Quality validation suite
- ‚úÖ `tests/benchmark.py` - Performance benchmarks
- ‚úÖ `docs/REPLICATION_GUIDE.md` - Course replication instructions
- ‚úÖ `README.md` - Complete project documentation
- ‚úÖ `docs/FINAL_VALIDATION.txt` - Validation results
- ‚úÖ `docs/FINAL_BENCHMARK.txt` - Performance metrics

### Success Criteria

- ‚úÖ 80%+ validation queries pass quality checks
- ‚úÖ Query latency <3 seconds (ideally <1s)
- ‚úÖ All documentation complete and accurate
- ‚úÖ Replication guide tested (mentally validated or on subset)
- ‚úÖ README provides clear project overview
- ‚úÖ Can demonstrate working system end-to-end

### Rollback Plan

If validation fails:

1. Review failed test cases in `docs/FINAL_VALIDATION.txt`
2. Identify root cause (chunking? embedding? reranking?)
3. Adjust parameters in respective modules
4. Rerun validation

### Time Estimate: 2-3 hours

---

## üìä Phase Completion Checklist

Use this to track progress through all phases:

### Pre-Implementation (Day 0)

- [ ] Repository structure explored
- [ ] Sample files inspected
- [ ] Project workspace created
- [ ] Git initialized

### Phase 1: SRT Parsing (Day 1)

- [ ] `src/srt_parser.py` implemented
- [ ] Tests pass on sample files
- [ ] Content quality validated
- [ ] Chunking strategy confirmed

### Phase 2: Embedding Model (Day 2)

- [ ] Models benchmarked
- [ ] Model selected and documented
- [ ] Speed/quality trade-off validated
- [ ] Can embed full course in <30 min

### Phase 3: LanceDB Integration (Day 3)

- [ ] `src/srt_processor.py` implemented
- [ ] `src/build_index.py` working
- [ ] Test indexing succeeds
- [ ] Full indexing completes
- [ ] Sample query returns results

### Phase 4: Query Engine (Day 4)

- [ ] `src/query_engine.py` implemented
- [ ] Reranking improves results
- [ ] Query latency acceptable
- [ ] Test suite passes

### Phase 5: User Interfaces (Days 5-6)

- [ ] CLI interface working
- [ ] Streamlit web UI functional
- [ ] Both interfaces tested
- [ ] Usage guide complete

### Phase 6: Validation (Day 7)

- [ ] Validation suite created
- [ ] Performance benchmarked
- [ ] Replication guide written
- [ ] README complete
- [ ] Final validation passed

---

## üéØ Success Criteria (Overall Project)

**Minimum Viable Product (MVP):**

- [x] Can process `.srt` files from course structure
- [x] Generates embeddings using local/free model
- [x] Stores embeddings in vector database
- [x] Accepts natural language queries
- [x] Returns top 5-10 relevant results with timestamps
- [x] Results demonstrably better than keyword search
- [x] Complete setup takes <1 day to replicate

**Validation Metrics:**

- **Search Quality**: 80%+ validation queries return relevant results
- **Performance**: Query latency <3 seconds
- **Completeness**: All documentation exists and accurate
- **Replicability**: Can apply to new course in <2 hours

---

## üîÑ Rollback & Contingency Plans

### If Full Indexing Fails (Phase 3)

**Option A**: Use Google Colab

1. Generate embeddings in Colab (GPU acceleration)
2. Export as Arrow format
3. Import into local LanceDB

**Option B**: Reduce Scope

1. Index only specific sections
2. Validate on subset
3. Expand gradually

### If Query Performance Unacceptable (Phase 4)

**Option A**: Disable Reranking

- Still get good results from vector search alone
- Reduces latency by ~50%

**Option B**: Lighter Models

- Use `all-MiniLM-L6-v2` (faster embedding)
- Skip cross-encoder entirely

### If Memory Issues

**Option A**: Batch Processing

```python
# Process in chunks of 50 files
for batch in chunks(all_files, 50):
    build_index(files=batch)
```

**Option B**: Reduce Dimensions

- Use 256-dim model instead of 384
- Smaller vectors = less memory

---

## üìà Post-Implementation Roadmap

After completing all 6 phases, consider:

### Week 2: Enhancements

- [ ] Add Jupyter notebook search
- [ ] Extract PDF slide content
- [ ] Implement query history

### Week 3: Optimization

- [ ] Fine-tune chunk sizes
- [ ] A/B test different embedding models
- [ ] Optimize reranking parameters

### Week 4: Polish

- [ ] Better error handling
- [ ] Logging and monitoring
- [ ] Auto-backup database

### Month 2: Scale

- [ ] Apply to other courses
- [ ] Build course comparison features
- [ ] Share as open-source tool

---

**End of Implementation Plan**

---
